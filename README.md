# Data Engineering Nano degree - PostgreSQL Data Warehouse

For this project, we were requested to create a very basic ETL project to read JSON files containing data about users and music listening from Sparkify. A data warehouse structure was requested using ***star*** schema.
This data warehouse could have many uses, like identify users patterns and provide summarised data.

## Project Execution

The right order to execute this project is:
1. `create_tables.py`
2. `etl.py`

The scripts have to be executed on this order for successfully execution. The `test.ipynb` notebook is of optional execution and should be executed after the whole process is finished. `sql_queries.py` is not required to be executed as it is imported by the two main scripts.

## Files Structure
### Scripts
#### `sql_queries.py`
This script is in charge of the SQL statements definition. It has to be imported by others script in order to load the scripts. This is a good practice to isolate queries from the ETL process. The SQL themselves will be covered on a later section of this file.

Some queries are stored on arrays according to its own use. Two arrays are created: one for tables creation and another one for tables dropping. One single query is not included on any array: `song_select` which is a SQL statement to query songs and artists ID by Artist name, Song title and Song duration.

#### `create_tables.py`
This script is the one in charge of the database creation and tables dropping and creation. The following functions can be found on the script: 

`create_database()` creates a connection with the PostgreSQL instance, drops the database sparkifydb and recreates the database. That means that everytime that this function is executed, the whole database will be deleted and re-created from scratch.

`drop_tables(cur, conn)` receives the connection and cursor and execute the scripts that are included on the array  drop_table_queries that is generated by the `sql_queries.py` script.

`create_tables(cur, conn)` also accepts as parameters the cursor and the database connection and executes the creation of table statements. These scripts are included on create_table_queries array that was defined on `sql_queries.py`.

`main()` is entry function and sets how the process occurs when the script is executed. The scope is create the database, drop the tables and create the tables.

#### `etl.py`
This is the script that executes the ETL job logic itself. Its job is to iterate the files according to the data directories given, load the JSON files, process the data and save the result on the database. The following functions are found on the script.

`process_data(cur, conn, filepath, func)` requires the following parameters: database cursor and connection, filepath and function to be applied on each file, as it could vary according to the type of the data. The filepath is not a absolute path but a folder that may contains songs or log data.

The function will iterate on the given directory and invoke the given function, that could be `process_song_file` or `process_log_file`. In order to follow the iteration, some messages are printed on the console to identify the total number of files to iterate and the current file.

`process_song_file(cur, filepath)` requires as parameters the database cursor and the absolute filepath. The purpose of this function is simply to read the JSON file that is provided, split artist and song data and insert the data on the database.
There is little data manipulation in place, which only purpose is to filter the columns from the dataset that are required for each table. 

`process_log_file(cur, filepath)` requires as parameters the database cursor and the absolute filepath. This table  will read the data from the JSON file, do some data manipulation and populate three tables: users, times and songplays.
The data manipulation includes:
1. Filtering records that have page equals to NextSong, as those are the effective "song plays" as documented.
2. Converting the ts column to humean reading data.
3. Creating the `time_df` Dataframe, which is going to be inserted as a dimension of times including human reading attributes about specific timestamps.
4. Creating the `users_df`, which has information about users.
5. Iterating on each "songplay" record, request the artist and song ID according to the artist name, song title and song duration.
6. Associating the Artist and Song ID with the record.
7. Creatint the `songplay_data` dataframe, that is going to be the fact table. It includes a surrogate key, compound of the concatenation of timestamp + userId. This is a feature to avoid duplication of data as the original dataset doesn't contain any valid identifier column.

`main()` is entry function and sets how the process occurs when the script is executed. The scope here is to connect to the database, create a cursor, process song's data, process log's data and close the connection.

### Notebooks

#### `etl.ipynb`
That's just a notebook to explore the operation of the ETL process from a single file point of view. It's not a mandatory execution.

#### `test.ipynb`
This notebook's goal is to check the data that is stored on the database and it's not a necessary execution. If desired to check the records, it should be executed after the ETL process is finished.

## Tables 

### Tables Definition

As requested, five different tables were created.

#### **songplays**
This table stores information about songs that were executed by Sparkify users. For each song that was played, a record is created. That's a ***fact*** table. The primary key is the songplay_id which is going to be a *surrogate key* compound by songplay's timestamp + User ID.

#### **users**
That is the users' dimension table, where will be stored important info like User ID, Name and Gender are going to seat here. The primary key is the user_id, which is provided on the JSON files.

#### **songs**
This table stores information about the songs - It's a dimension. with information about songs. The primary key will be the song_id, provided by the JSON files.

#### **artists**
That table stores artists' data and will be directly by songs. It's a dimension and the primary key is going to be the artist_id, information that is included on the Sparkify's JSON files.

#### **time**
That's another dimension table. It contains detailed information about every timestamp present on *songplays* table. The primary key is going to be timestamp.

These DDL stataments are defined on the `sql_queries.py` script.

### Tables Insertion
The process to insert the records is defined on the `sql_queries.py`. The tables insert script follows the columns according to the table definition.

The statements are actually ***upserts*** as the condition `ON CONFLICT` was used to ignore duplicates records, based on the primary key.

# Sample queries

- Total song executions buy hour of day

~~~~sql
SELECT time.hour,
       count(*)
FROM songplays
JOIN TIME ON songplays.start_time = time.timestamp
GROUP BY time.hour
ORDER BY time.hour;
~~~~

- Top 5 Paid Users Listeners summary
~~~~sql
SELECT top_5.*,
       users.first_name,
       users.last_name,
       users.gender
FROM
  (SELECT users.user_id,
          count(*) AS executions
   FROM songplays
   JOIN users ON users.user_id = songplays.user_id
   AND users.level = 'paid'
   GROUP BY users.user_id
   ORDER BY count(*) DESC
   LIMIT 5) top_5
JOIN users ON top_5.user_id = users.user_id;
~~~~